{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this notebook: \n",
        "to train PyTorch mask-rcnn network for detection of russian car plates"
      ],
      "metadata": {
        "id": "D3NzIl6dPjsL"
      },
      "id": "D3NzIl6dPjsL"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyheif"
      ],
      "metadata": {
        "id": "VnfGZ1W5-1t5"
      },
      "id": "VnfGZ1W5-1t5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5cae692",
      "metadata": {
        "cellId": "5s3z3twuvc8pn6dm6m6t9",
        "id": "c5cae692"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import pyheif\n",
        "import json\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "import torchvision\n",
        "from torchvision.io import read_image\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as F\n",
        "import skimage\n",
        "from skimage import draw\n",
        "from torchvision.utils import draw_segmentation_masks, draw_bounding_boxes\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.ops import box_iou\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9856fa",
      "metadata": {
        "cellId": "0ry8ipbohlyhjx0g6l4ioi",
        "execution_id": "0b7f188b-dd54-4975-b7de-1fe696aa4018",
        "id": "7a9856fa"
      },
      "source": [
        "## Download data in storage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZZcvVqBC_Blu"
      },
      "id": "ZZcvVqBC_Blu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d94dcb",
      "metadata": {
        "cellId": "6oqofmll8nur5bjixojswf",
        "id": "a0d94dcb"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/Carplate_dataset2/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06cd8369",
      "metadata": {
        "cellId": "x4u40d0y493zrng3qcaxq",
        "execution_id": "91f1fece-469d-4001-b392-0e82fa470e5f",
        "id": "06cd8369"
      },
      "source": [
        "## Description of data format for mask-crnn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bf32f34",
      "metadata": {
        "cellId": "d1jp27l6eh9qhe4kbtiw2",
        "execution_id": "380843c5-6193-4eb5-9c29-24ef1629e6ea",
        "id": "8bf32f34"
      },
      "source": [
        "The only specificity that we require is that the dataset __getitem__ should return:\n",
        "\n",
        "> image: a PIL Image of size (H, W)\n",
        "\n",
        "> target: a dict containing the following fields:\n",
        "\n",
        "1.  boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with 0 <= x1 < x2 <= W and 0 <= y1 < y2 <= H. \n",
        "2.  labels (Int64Tensor[N]): the class label for each ground-truth box\n",
        "3.  masks (UInt8Tensor[N, H, W]): the segmentation binary masks for each instance\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc9d0965",
      "metadata": {
        "cellId": "l9ddoq9qvrw515wcsqwm",
        "execution_id": "014ba129-e158-42cc-959c-4d2847f69482",
        "id": "cc9d0965"
      },
      "source": [
        "## Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a413c16",
      "metadata": {
        "cellId": "z7miiae4owqheamjsmq9o",
        "id": "2a413c16"
      },
      "outputs": [],
      "source": [
        "class Carplate_Dataset(Dataset):\n",
        "    def __init__(self, data_path, transforms):\n",
        "        self.data_path = data_path\n",
        "        self.annotations = self.get_annotations(os.path.join(data_path, 'via_region_data.json'))['_via_img_metadata']\n",
        "        self.images = list(set(os.listdir(data_path)) & set(self.annotations.keys()))\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def get_annotations(self, path_to_file):\n",
        "        annotations_dict = json.load(open(path_to_file))\n",
        "        return annotations_dict\n",
        "        \n",
        "    def get_bboxes_and_labels(self, idx):\n",
        "        img_annotations = self.annotations[self.images[idx]]\n",
        "        boxes = []\n",
        "        for r in img_annotations['regions']:\n",
        "            polygon = r['shape_attributes']\n",
        "            if polygon.get('all_points_x') is None or polygon.get('all_points_y') is None:\n",
        "                continue\n",
        "            boxes.append([min(polygon['all_points_x']), min(polygon['all_points_y']),\n",
        "                        max(polygon['all_points_x']), max(polygon['all_points_y'])])\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float64)\n",
        "        labels = torch.tensor([1 for i in range(boxes.size(0))], dtype=torch.int64)\n",
        "        return boxes, labels\n",
        "    \n",
        "    def get_masks(self, idx, w, h):\n",
        "        polygons = [r['shape_attributes'] \n",
        "                    for r in self.annotations[self.images[idx]]['regions']]\n",
        "        masks = np.zeros([len(polygons), h, w], dtype=np.uint8)\n",
        "        for i, polygon in enumerate(polygons):\n",
        "            if polygon.get('all_points_x') is None or polygon.get('all_points_y') is None:\n",
        "                continue\n",
        "            rr, cc = draw.polygon(polygon['all_points_y'], polygon['all_points_x'], (h,w))\n",
        "            masks[i, rr, cc] = 1\n",
        "        masks = torch.tensor(masks, dtype=torch.bool)\n",
        "        return masks\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data_path + self.images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        width = img.size[0]\n",
        "        height = img.size[1]  \n",
        "        \n",
        "        image_id = torch.tensor([idx],dtype=torch.int64)\n",
        "        boxes, labels = self.get_bboxes_and_labels(idx)\n",
        "        masks = self.get_masks(idx, width, height)\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        target['image_id'] = image_id\n",
        "        target['masks'] = masks\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "        \n",
        "        return img, target\n",
        "    \n",
        "    def vizualize_masks(self, idx):\n",
        "        img_path = self.data_path + self.images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        transform = T.PILToTensor()\n",
        "        img_tensor = transform(img)\n",
        "        \n",
        "        masks = self.get_masks(idx, img.size[0], img.size[1])\n",
        "\n",
        "        transform = T.ToPILImage()\n",
        "        annotated_img = transform(draw_segmentation_masks(img_tensor, masks))\n",
        "        annotated_img.show()\n",
        "    \n",
        "    def vizualize_boxes(self, idx):\n",
        "        img_path = self.data_path + self.images[idx]\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        transform = T.PILToTensor()\n",
        "        img_tensor = transform(img)\n",
        "        \n",
        "        boxes, _ = self.get_bboxes_and_labels(idx)\n",
        "        \n",
        "        transform = T.ToPILImage()\n",
        "        annotated_img = transform(draw_bounding_boxes(img_tensor, boxes))\n",
        "        annotated_img.show()\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f04dc1bd",
      "metadata": {
        "cellId": "63cfabo65utjpqc6xb0h8",
        "execution_id": "3f83baa4-c01e-43f2-8442-3da69b7ed2ed",
        "id": "f04dc1bd"
      },
      "source": [
        "## Define transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8a227ed",
      "metadata": {
        "cellId": "ey3yml1jrqdrpuwh5k3zv",
        "id": "b8a227ed"
      },
      "outputs": [],
      "source": [
        "class Compose():\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "    \n",
        "class ImageToTensor():\n",
        "    def __call__(self, image, target):\n",
        "        transform = T.ToTensor()\n",
        "        image = transform(image)\n",
        "        return image, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7e5e1e7",
      "metadata": {
        "cellId": "v4k9wkkxd4me6u8q1ldmcv",
        "id": "d7e5e1e7"
      },
      "outputs": [],
      "source": [
        "data_transforms = [ImageToTensor()]\n",
        "transforms_sequence = Compose(data_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaf74356",
      "metadata": {
        "cellId": "cp9j0t0jgdmq3er5xknh9q",
        "execution_id": "e19fc294-353f-4b35-bdab-44ea731904ff",
        "id": "aaf74356"
      },
      "source": [
        "## Create Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0217e1e2",
      "metadata": {
        "cellId": "r6xqb019ati3d2cqbnqkab",
        "id": "0217e1e2"
      },
      "outputs": [],
      "source": [
        "train_dataset = Carplate_Dataset(os.path.join(data_path, 'train/'), transforms_sequence)\n",
        "val_dataset = Carplate_Dataset(os.path.join(data_path, 'val/'), transforms_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee28d2cd",
      "metadata": {
        "cellId": "qsqcrp58vlm8c36qxslbqt",
        "id": "ee28d2cd"
      },
      "outputs": [],
      "source": [
        "def collate_function(batch):\n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebf61695",
      "metadata": {
        "cellId": "c5pe9en2k9ui68k39b2rl",
        "id": "ebf61695"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True,\n",
        "                        collate_fn=collate_function)\n",
        "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=True,\n",
        "                        collate_fn=collate_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14017070",
      "metadata": {
        "cellId": "ejzc6lboygkrrhlkjk4ca",
        "execution_id": "e7be881c-aab0-40d9-bc39-a7bb53ecfb28",
        "id": "14017070"
      },
      "source": [
        "## Train function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neptune-client"
      ],
      "metadata": {
        "id": "X3kUFWUnPmWm"
      },
      "id": "X3kUFWUnPmWm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb0d820c",
      "metadata": {
        "cellId": "rdax36z6oesiu6agkzfz69",
        "id": "eb0d820c"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "import torch.optim as optim\n",
        "from tqdm import trange\n",
        "from tqdm import tqdm\n",
        "import neptune.new as neptune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b21a792",
      "metadata": {
        "cellId": "fdnjv6gr8q5zwnylzn8ohk",
        "id": "5b21a792"
      },
      "outputs": [],
      "source": [
        "def fit_epoch(model, train_loader, optimizer, lr_scheduler):\n",
        "    model.train()\n",
        "    device = torch.device(\"cuda\")\n",
        "    num_batches = 0\n",
        "    total_loss = 0\n",
        "    torch.cuda.empty_cache()\n",
        "    for _, batch in enumerate(tqdm(train_loader)):\n",
        "        images, targets = batch\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "        num_batches += 1\n",
        "        run[\"train/loss\"].log(float(losses))\n",
        "        run[\"train/loss_classifier\"].log(float(loss_dict['loss_classifier']))\n",
        "        run[\"train/loss_box_reg\"].log(float(loss_dict['loss_box_reg']))\n",
        "        run[\"train/loss_mask\"].log(float(loss_dict['loss_mask']))\n",
        "        run[\"train/loss_objectness\"].log(float(loss_dict['loss_objectness']))\n",
        "        run[\"train/loss_rpn_box_reg\"].log(float(loss_dict['loss_rpn_box_reg']))\n",
        "        total_loss += float(losses)\n",
        "        \n",
        "        \n",
        "    return total_loss/num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021c04fb",
      "metadata": {
        "cellId": "kaqdtdevasbvm986chctn9",
        "id": "021c04fb"
      },
      "outputs": [],
      "source": [
        "def get_IoU_metric(target_masks, predicted_masks):\n",
        "    if target_masks.size(0)>1:\n",
        "      return 0\n",
        "    intersection = torch.sum(target_masks*predicted_masks, (1,2))\n",
        "    intersection = torch.sum(intersection)/len(intersection)\n",
        "    union = torch.sum(target_masks+predicted_masks, (1,2))\n",
        "    union = torch.sum(union)/len(union)\n",
        "    IoU_metric=intersection/union\n",
        "    return float(IoU_metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcfe3cdf",
      "metadata": {
        "cellId": "r9y7w37p2j8fs255iytzdg",
        "id": "fcfe3cdf"
      },
      "outputs": [],
      "source": [
        "def eval_epoch(model, val_loader):\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\")\n",
        "    num_batches = 0\n",
        "    total_iou_masks = 0\n",
        "    total_iou_boxes = 0\n",
        "    torch.cuda.empty_cache()\n",
        "    for _, batch in enumerate(tqdm(val_loader)):\n",
        "        images, targets = batch\n",
        "        images = [image.to(device) for image in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output = model(images)\n",
        "        iou_masks_per_batch = np.mean([get_IoU_metric(targets[i]['masks'], output[i]['masks'][:,0,:,:]) \n",
        "                             for i in range(len(output))])\n",
        "        iou_boxes_per_batch = np.mean([float(torch.sum(box_iou(output[i]['boxes'], targets[i]['boxes'])) / output[i]['boxes'].size(0))\n",
        "                             for i in range(len(output))])\n",
        "        run['eval/IoU_for_masks'].log(iou_masks_per_batch)\n",
        "        run['eval/IoU_for_boxes'].log(iou_boxes_per_batch)\n",
        "        total_iou_masks += iou_masks_per_batch\n",
        "        total_iou_boxes += iou_boxes_per_batch\n",
        "        num_batches += 1\n",
        "        \n",
        "    return total_iou_masks/num_batches, total_iou_boxes/num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d4dc50",
      "metadata": {
        "cellId": "6ipeode8m1vlz7lxejnm4",
        "id": "27d4dc50"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, val_dataloader, model, epochs, opt, lr_scheduler):\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)\n",
        "    for epoch in trange(epochs, desc=\"Epoch:\"):\n",
        "        linearLR_scheduler = None\n",
        "        if epoch == 0:\n",
        "           warmup_factor = 1.0 / 1000\n",
        "           warmup_iters = min(1000, len(train_dataloader) - 1)\n",
        "\n",
        "           linearLR_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "               optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
        "           )\n",
        "        train_loss = fit_epoch(model, train_dataloader, opt, linearLR_scheduler)\n",
        "        lr_scheduler.step()\n",
        "        iou_masks, iou_boxes = eval_epoch(model, val_dataloader)\n",
        "        run['train/epoch/loss'].log(train_loss)\n",
        "        run['eval/epoch/iou_masks'].log(iou_masks)\n",
        "        run['eval/epoch/iou_boxes'].log(iou_boxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61afa426",
      "metadata": {
        "cellId": "8c0j5dnwlef3l7vb08sbxl",
        "execution_id": "6d0fc731-d4f8-4c2c-8add-ba76d43d74e9",
        "id": "61afa426"
      },
      "source": [
        "## Load model and change number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b7126cf",
      "metadata": {
        "cellId": "s27ox0no6oh7xuoizc7c1b",
        "id": "6b7126cf"
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "num_classes = 2\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "hidden_layer = 256\n",
        "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "#loading pretrained model\n",
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/mask-rcnn_pretrainedv2_1_carplate_dataset.pt', map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "qS21tEaI_rZ8"
      },
      "id": "qS21tEaI_rZ8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bf2ed456",
      "metadata": {
        "cellId": "5lh3950euau66nqg74vg0q",
        "execution_id": "ea43316b-59f4-43ae-bf50-d495fc7b2f10",
        "id": "bf2ed456"
      },
      "source": [
        "## Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a627351",
      "metadata": {
        "cellId": "9pcdo6q76kk10y4u3zpl9b",
        "id": "8a627351"
      },
      "outputs": [],
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.003,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                   step_size=2,\n",
        "                                                   gamma=0.1)\n",
        "num_epochs = 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5946b1e0",
      "metadata": {
        "cellId": "zlbmteldbyiy273e131ef",
        "id": "5946b1e0"
      },
      "outputs": [],
      "source": [
        "run = neptune.init(\n",
        "    api_token= os.getenv('NEPTUNE_API_TOKEN'),\n",
        "    project='misha/carplate-segmentation'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0f9b52a",
      "metadata": {
        "cellId": "9n18hhefxglcmpgniek26t",
        "id": "a0f9b52a"
      },
      "outputs": [],
      "source": [
        "train(train_loader, val_loader, model, num_epochs, optimizer, lr_scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffb62f37",
      "metadata": {
        "cellId": "85yo8rhmraqv5uqi85s32",
        "id": "ffb62f37"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/mask-rcnn_pretrainedv2_2_carplate_dataset.pt')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "notebookId": "17050f52-4abf-4e03-a179-b4a338bb491e",
    "notebookPath": "instance_segm.ipynb",
    "colab": {
      "name": "Carplates_detector.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}